# -*- coding: utf-8 -*-
"""외계 행성 찾기 및 거주 가능성 판별

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dfTcr4BEvyn2SRrTk8gDubgchhUg7BtV

# **1. 외계행성 생존 가능성 머신러닝 학습**

# 오버샘플링
"""

import tensorflow as tf
import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

full_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/project/phl_exoplanet_catalog_2019.csv')

import os

os.chdir('/content/drive/MyDrive/Colab Notebooks/project')

full_data

full_data.info()

full_data.columns

# P_HABITABLE = 행성은 잠재적으로 거주 가능 지수입니다(1 = 보수적, 2 = 낙관적).
import matplotlib.pyplot as plt
fig = plt.figure(figsize = (9,6))
full_data.P_HABITABLE.value_counts(normalize = True, ascending = False).plot(kind='bar', color= ['navy','orange','green'], alpha = 0.8, rot=0)
# value_counts(normalize = True, ascending = False): 각 등급별 비율, 내림차순
plt.title('거주불가 (0) / 보수적으로 가능 (1) / 낙관적으로 가능 (2) in the Imbalanced Dataset')
plt.show()

full_data['P_HABITABLE'].value_counts(normalize=True)

#간단한 오버샘플링 전략으로 클래스 불균형 해결
#확실히 데이터 세트의 98.64%가 거주 가능한 행성으로 인해 불균형이 발생했습니다.
#각각 0.84%와 0.52%만이 보수적으로 거주 가능한 행성이고 낙관적으로 거주 가능한 행성입니다.
#ML 모델의 적절한 성능을 얻으려면 먼저 각 클래스가 동일한 표현 비율을 갖는 데이터 세트의 균형을 맞춰야 합니다.
#이를 위해 간단한 오버샘플링 기술(리샘플링 전략)을 사용하고 있습니다.

from sklearn.utils import resample
no = full_data[full_data.P_HABITABLE == 0]
yes_cons = full_data[full_data.P_HABITABLE == 1]
yes_opti = full_data[full_data.P_HABITABLE == 2]
# 사이킷런에서 resampling을 통해 소수의 클래스를 추출해서 클래스 비율을 맞춤
# 사이킷런의 resample은 데이터셋에서 중복을 허용해서 샘플을 추출함
#
# X, y = resample(샘플링할 X, 샘플링할 y, replace=True, n_samples=100) 100번될때까지 무작위로 샘플링해 반환
yes_cons_oversampled = resample(yes_cons, replace=True, n_samples=len(no), random_state=12345)
# df_12_axis0 = pd.concat ([df_1, df_2]) # 행 바인딩: 축 = 0, 기본값
# 행을 따라 DataFrame1, 2를 연결합니다. 축=0, 기본값
oversampled = pd.concat([no, yes_cons_oversampled])
yes_opti_oversampled = resample(yes_opti, replace=True, n_samples=len(no), random_state=12345)
oversampled = pd.concat([oversampled, yes_opti_oversampled])

fig = plt.figure(figsize = (9,6))
oversampled.P_HABITABLE.value_counts(normalize = True, ascending = False).plot(kind='bar', color= ['navy','orange','green'], alpha = 0.8, rot=0)
plt.title('거주불가 (0) / 보수적으로 가능 (1) / 낙관적으로 가능 (2) after Oversampling (Balanced Dataset)')
plt.show()

oversampled['P_HABITABLE'].value_counts(normalize=True)

"""# 데이터 전처리"""

# heatmap: 열을 의미하는 heat와 지도를 뜻하는 map을 합친 단어로
#          데이터들의 배열을 색상으로 표현해주는 그래프이다.
# heatmap을 사용하는 이유
# 1. 두개의 카테고리 값에 대한 값 변화를 한눈에 알기 쉽다.
# 2. 대용량 데이터도 heatmap을 이용해 시각화 한다면 이미지 몇장으로 표현이 가능하다.

# Missing Data Pattern in Training Data
import seaborn as sns
# isnull: 관측치가 결측(missing value)이면 True, 결측이 아니면 False
sns.heatmap(oversampled.isnull(), cbar=False, cmap='PuBu') # , colorbar의 유무, 히트맵의 색

total = oversampled.isnull().sum().sort_values(ascending=False) # null값 개수 합 내림차순으로 정렬
percent = (oversampled.isnull().sum()/oversampled.isnull().count()).sort_values(ascending=False)
missing = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing.head(50)

# 'P_GEO_ALBEDO'부터 'P_MASS'까지 모든 특성에 누락된 값이 50% 이상 있음을 알 수 있다.
# 따라서 편견을 피하기 위해 결측치가 50%이상인 경우 해당 기능을 폐기하겠습니다.

compact_data = oversampled.drop(['P_GEO_ALBEDO', 'P_DETECTION_MASS', 'P_DETECTION_RADIUS', 'P_ALT_NAMES', 'P_ATMOSPHERE', 'S_DISC', 'S_MAGNETIC_FIELD',
                 'P_TEMP_MEASURED', 'P_GEO_ALBEDO_ERROR_MIN', 'P_GEO_ALBEDO_ERROR_MAX', 'P_TPERI_ERROR_MAX', 'P_TPERI_ERROR_MIN', 'P_TPERI',
                 'P_DENSITY', 'P_ESCAPE', 'P_GRAVITY', 'P_POTENTIAL', 'P_OMEGA_ERROR_MAX', 'P_OMEGA_ERROR_MIN', 'P_OMEGA', 'P_INCLINATION_ERROR_MAX',
                 'P_INCLINATION_ERROR_MIN', 'P_INCLINATION', 'P_ECCENTRICITY_ERROR_MAX', 'P_ECCENTRICITY_ERROR_MIN', 'S_AGE_ERROR_MIN', 'S_AGE_ERROR_MAX',
                 'P_IMPACT_PARAMETER_ERROR_MIN', 'P_IMPACT_PARAMETER_ERROR_MAX', 'P_IMPACT_PARAMETER', 'P_MASS_ERROR_MAX', 'P_MASS_ERROR_MIN', 'P_HILL_SPHERE',
                 'P_MASS'], axis = 1)

compact_data.info()

# 컴팩트 데이터 세트에는 77개의 기능이 있습니다.
# 이제 결측값이 있는 범주형 열을 식별하고 먼저 모드로 대치하겠습니다.

compact_data.select_dtypes(include=['object']).columns

compact_data_obj = compact_data.select_dtypes(include=['object'])

total = compact_data_obj.isnull().sum().sort_values(ascending=False)
percent = (compact_data_obj.isnull().sum()/compact_data_obj.isnull().count()).sort_values(ascending=False)
missing = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing.head()

# fillna: DataFrame에서 결측값을 원하는 값으로 변경하는 메서드
# mode(): 각 열에 대해서 최빈값이 인덱스 0에 출력된다.만약 최빈값이 여러개일 경우 갯수만큼 인덱스가 생성되어 출력된다.
compact_data['S_TYPE'] = compact_data['S_TYPE'].fillna(compact_data['S_TYPE'].mode()[0])
compact_data['P_TYPE_TEMP'] = compact_data['P_TYPE_TEMP'].fillna(compact_data['P_TYPE_TEMP'].mode()[0])
compact_data['S_TYPE_TEMP'] = compact_data['S_TYPE_TEMP'].fillna(compact_data['S_TYPE_TEMP'].mode()[0])
compact_data['P_TYPE'] = compact_data['P_TYPE'].fillna(compact_data['P_TYPE'].mode()[0])
compact_data.head()

# 이제 레이블 인코딩(문자열 값을 숫자형 카테고리 값으로 변경)을 사용하여 범주형 열을 숫자형 열로 변환하겠습니다.

# 머신러닝을 위한 대표적인 인코딩 방식은 레이블 인코딩(Label Encoding)과 원-핫 인코딩이 있습니다.
# 사이킷런의 알고리즘은 문자열 값을 입력 값으로 허용하지 않습니다. 그래서 모든 문자열 값은 인코딩 돼서 숫자 형으로 변환해야 합니다.
# 레이블 인코딩은 간단하게 문자열 값을 숫자형 카테고리 값으로 변환합니다.
# 하지만 레이블 인코딩이 일괄적으로 숫자 값으로 변환이 되면서 몇몇 ML알고리즘에는 이를 적용할 경우 예측 성능이 떨어지는 경우가 발생할 수 있습니다.
# 숫자로 되어 있어 잘못하면 가중치로 인식하여 값에 왜곡이 생기게 됩니다.
# 이러한 특성 때문에 레이블 인코딩은 선형 회귀와 같은 ML알고리즘에는 적용하지 않습니다.
# 트리 계열의 ML 알고리즘은 숫자의 이러한 특성을 반영하지 않으므로 레이블 인코딩도 별문제가 없습니다.

# Label Encoding을 사용하여 범주형 특성을 연속형 특성으로 변환
from sklearn.preprocessing import LabelEncoder
lencoders = {}
for col in compact_data.select_dtypes(include=['object']).columns:
    lencoders[col] = LabelEncoder()
    compact_data[col] = lencoders[col].fit_transform(compact_data[col])

# 다음으로, MICE 패키지를 사용하여 전체 데이터 세트(앞서 모드를 사용하여 범주형 데이터를 대치했기 때문에 실제로는 숫자 값만)에 대한 결측값을 대치하겠습니다.

# Jupyternotebook(또는 ipython)에서 경고 메시지를 무시하고 싶을 때:
import warnings
warnings.filterwarnings("ignore")

# 연결 방정식에 의한 다중 대치
# https://leehah0908.tistory.com/5
# MICE를 이용한 자동 대치
# - Round robin 방식을 반복하여 결측 값을 회귀하는 방식으로 결측치를 처리합니다.
# - 결측 값을 회귀하는 방식으로 처리하기 때문에 이 방식은 수치형 변수에 자주 사용합니다.
# - 범주형 변수에도 사용이 가능하지만 조금 더 복잡하고 먼저 인코딩됩니다.
from sklearn.experimental import enable_iterative_imputer # iterativeimputer(파라미터)
from sklearn.impute import IterativeImputer # IterativeImputer (회귀대치)
# 얕은 복사는 객체를 새로운 객체로 복사하지만 원본 객체의 주소값을 복사하는 것이고, 깊은 복사는 전체 복사로 참조값의 복사가 아닌 참조된 객체 자체를 복사하는 것을 말한다.
# 코딩을 하다보면 원본 배열의 보존을 할 필요가 허다하기 때문에 이럴때는 배열을 '깊은 복사' 하여야 한다.
MiceImputed = compact_data.copy(deep=True)
mice_imputer = IterativeImputer() # IterativeImputer(random_state=83)
MiceImputed.iloc[:, :] = mice_imputer.fit_transform(compact_data)

MiceImputed.head()

# isna(): NaN값 포함 여부를 Boolean 타입의 값으로 반환
# isna().sum(): 결측치 개수
MiceImputed.isna().sum(axis = 0)

# 다중공선성(통계학의 회귀분석에서 독립변수들 간에 강한 상관관계가 나타나는 문제이다.)제거 :
# 다음으로, 특성 쌍 간에 완벽한 상관 관계가 존재하는지 확인합니다. 다중 공선성을 피하기 위해 하나를 제외하고 해당 쌍에서 하나를 유지합니다.

# Correlation Heatmap
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
corr = MiceImputed.corr()
mask = np.triu(np.ones_like(corr, dtype=np.bool))
f, ax = plt.subplots(figsize=(20, 20))
cmap = sns.diverging_palette(250, 25, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=None, center=0,square=True, annot=False, linewidths=.5, cbar_kws={"shrink": 0.9})

# 이미지의 명확성을 유지하기 위해 위의 주석을 피했지만 진한 빨간색 사각형은 교차하는 특징 쌍 간의 완벽한 상관관계를 나타냅니다.
# 우리는 그것들을 폐기할 것입니다.

# 완벽하게 상관된 특성 삭제
working_data = MiceImputed.drop(['S_NAME', 'P_RADIUS', 'P_RADIUS_ERROR_MIN', 'P_RADIUS_ERROR_MAX', 'P_DISTANCE', 'P_PERIASTRON', 'P_APASTRON',
                                 'P_DISTANCE_EFF', 'P_FLUX_MIN', 'P_FLUX_MAX', 'P_TEMP_EQUIL', 'P_TEMP_EQUIL_MIN', 'P_TEMP_EQUIL_MAX',
                                 'S_RADIUS_EST', 'S_RA_H', 'S_RA_T', 'S_LUMINOSITY', 'S_HZ_OPT_MIN', 'S_HZ_OPT_MAX', 'S_HZ_CON_MIN',
                                 'S_HZ_CON_MAX', 'S_HZ_CON0_MIN', 'S_HZ_CON0_MAX', 'S_HZ_CON1_MIN', 'S_HZ_CON1_MAX', 'S_SNOW_LINE',
                                'P_PERIOD_ERROR_MIN', 'P_PERIOD_ERROR_MAX', 'S_MAG', 'S_DISTANCE_ERROR_MIN', 'S_DISTANCE_ERROR_MAX',
                                 'S_METALLICITY', 'S_METALLICITY_ERROR_MIN', 'S_METALLICITY_ERROR_MAX', 'S_AGE', 'S_TEMPERATURE_ERROR_MIN',
                                 'S_TEMPERATURE_ERROR_MAX', 'S_ABIO_ZONE', 'P_ESI', 'S_CONSTELLATION_ABR', 'P_SEMI_MAJOR_AXIS_EST'], axis=1)

working_data.info()

# 현재 작업 데이터 세트에는 36개의 기능만 있습니다.
# 이제 주석 "1"이 나타나는 사각형이 있는지 주석과의 상관 관계를 다시 한 번 교차 확인하겠습니다.

# 축소된 작업 데이터 세트에 대한 상관 히트맵
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
corr = working_data.corr()
mask = np.triu(np.ones_like(corr, dtype=np.bool))
f, ax = plt.subplots(figsize=(20, 20))
cmap = sns.diverging_palette(250, 25, as_cmap=True)
sns.heatmap(corr, mask=mask, cmap=cmap, vmax=None, center=0,square=True, annot=True, linewidths=.5, cbar_kws={"shrink": 0.9})

# 일부 높은 상관관계가 발견되었지만 완벽한 상관관계는 없습니다(예: "1").

# 이상값(이상한 관측값) 제거:
# 다음으로 IQR(Inter Quartile Range)을 사용하여 이상값을 식별하고 이를 제거하겠습니다.

# IQR이란, Interquartile range의 약자로써 Q3 - Q1를 의미한다.
# - Q3 - Q1: 사분위수의 상위 75% 지점의 값과 하위 25% 지점의 값 차이
# - Q1에서 1.5 * IQR을 한 값을 빼준 값을 최소 제한선, Q3에서 1.5 * IQR을 더해준 값을 최대 제한선으로 둔다.
# 그 밑, 또한 그것을 넘어가는 값들을 이상치라고 말할 수 있다.

# IQR을 사용하여 이상값 감지
# qauntile: 주어진 데이터를 동등한 크기로 분할하는 지점을 말합니다.
#           예를 들어, 시험을 치고 상위 10%에 속하는지를 확인하고 싶다면 점수 데이터를 정렬한 뒤 데이터의 수를 10개의 동등한 크기,
#           즉 각 그룹이 같은 데이터 수를 보유하도로 분할하면 됩니다.
Q1 = working_data.quantile(0.25)
Q3 = working_data.quantile(0.75)
IQR = Q3 - Q1
print(IQR)

# 데이터 세트에서 이상값 제거
working_data = working_data[~((working_data < (Q1 - 1.5 * IQR)) |(working_data > (Q3 + 1.5 * IQR))).any(axis=1)]

working_data.head()

"""# 변수 중요도 분석"""

# 변수 중요도 분석:
# 데이터 전처리가 완료된 후 외계 행성의 거주 가능성에 기여하는 매우 중요한 기능을 선택합니다.
# Random Forest를 사용한 순열 중요도와 Random Forest 및 Extra Trees 분류기를 사용한 래퍼 방법을 사용하겠습니다.

import warnings
warnings.filterwarnings("ignore")

# Permutation Importance는 기본적으로 제공되는 Feature Importance와 비슷한 기능을 한다.
# 기본적으로 제공되는 feature importance는 보통 가지에서 몇번 등장하는지, 혹은 불순도를 얼마나 낮추는 지에 대한 지표이다.
# 하지만 이 방법의 문제점은 -영향(negative)을 주는 feature를 알수 없다는 것이다.
# 즉, feature selection에서 가장 중요한 불필요한 feature를 빼는 과정에 있어서 크게 도움을 받을 수 없다.
# Permutation Importance는 모델을 학습시킨 뒤, 특정 feature의 데이터를 shuffle 했을 때, 검증 데이터 셋에 대한 예측성능을 확인하고 feature importance를 계산한다.
# 즉, shuffle 했을 때 모델의 성능이 떨어지면, 중요한 feature 인거고 성능이 그대로거나, 좋아지면 중요하지 않은 feature 이다.

# randomforest
# https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-5-%EB%9E%9C%EB%8D%A4-%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8Random-Forest%EC%99%80-%EC%95%99%EC%83%81%EB%B8%94Ensemble

!pip install eli5

import eli5
# https://hong-yp-ml-records.tistory.com/51
from eli5.sklearn import PermutationImportance
from sklearn.ensemble import RandomForestClassifier as rf

X = working_data.drop('P_HABITABLE', axis=1) # axis=0: 행(인덱스)을 따라 동작/ axis=1: 열(컬럼)을 따라 동작
y = working_data['P_HABITABLE']
perm = PermutationImportance(rf(n_estimators=20, random_state=0).fit(X,y),random_state=1).fit(X,y)
# 모델에서 사용할 트리 개수(학습시 생성할 트리 개수), 난수 seed 설정 -> 모델을 생성하고 위의 학습데이터를 학습(fit)시킨다.
eli5.show_weights(perm, feature_names = X.columns.tolist())
# columns.tolist(): 칼럼을 리스트로 변환

from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier as rf

#X = working_data.drop('P_HABITABLE', axis=1)
#y = MiceImputed['P_HABITABLE']
selector = SelectFromModel(rf(n_estimators=1000, random_state=0)) #  SelectFromModel에서 RandomForestClassifier를 입력하여 특성의 중요도를 기준으로 변수를 선택
selector.fit(X, y)
support = selector.get_support() # 선택한 특성을 불린 값으로 표시해주어 어떤 특성이 선택되었는지 확인할 수 있음
features = X.loc[:,support].columns.tolist()
print(features)
print(rf(n_estimators=1000, random_state=0).fit(X,y).feature_importances_)

from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import ExtraTreesClassifier as et

#X = working_data.drop('P_HABITABLE', axis=1)
#y = MiceImputed['P_HABITABLE']
selector = SelectFromModel(et(n_estimators=1000, random_state=123))
selector.fit(X, y)
support = selector.get_support()
features = X.loc[:,support].columns.tolist()
print(features)
print(et(n_estimators=100, random_state=123).fit(X,y).feature_importances_)

# 세 가지 방법 중 하나 이상의 방법으로 반복적으로 나타나는 칼럼을 중요한 칼럼으로 설정할 것

"""# test/train 분할"""

features = working_data[['P_TYPE_TEMP','P_PERIOD','S_DEC','S_DISTANCE','S_MASS','S_TEMPERATURE','P_TYPE','S_TIDAL_LOCK','P_HABZONE_OPT','P_RADIUS_EST']]
target = working_data['P_HABITABLE']

# Split into test and train
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.20, random_state=12345) # 분할시킬 데이터 입력,
# X_train, X_test, Y_train, Y_test : arrays에 데이터와 레이블을 둘 다 넣었을 경우의 반환이며, 데이터와 레이블의 순서쌍은 유지된다.
# feature 스케일링
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

"""# 지도학습"""

#지도 학습을 통한 모델링:
#다중 클래스 분류 문제를 위해 모든 분류기에 대해 one-vs-rest 전략을 사용
#이 전략은 다중 클래스 분류 문제를 여러 이진 분류 문제로 분해한 다음 결과를 비교/결합함.
#분류 목적으로 다음 모델이 사용된다.

#[참조한 코드 대상자가 사용한 방법]
#- Ridge Penalty를 사용한 로지스틱 회귀
#- 올가미 페널티를 사용한 확률적 경사하강법
#- 다항식 나이브 베이즈
#- 힌지 손실이 있는 수동적 공격적 분류기
#- 퍼셉트론
#- 그라디언트 부스팅 분류기

#[내가 생각한 방법]
#- 로지스틱
#- 나이브 베이즈
#- K-NN
#- 결정트리
#- 퍼셉트론

"""# 로지스틱 회귀 분석"""

# 1. 로지스틱 회귀 분석
from sklearn.linear_model import LogisticRegression

l_model = LogisticRegression()
l_model.fit(X_train, y_train)

l_model.score(X_train, y_train)

l_model.score(X_test, y_test)

l_model.coef_

"""# 나이브 베이즈(gaussian)

"""

from sklearn.naive_bayes import GaussianNB
#Create a Gaussian Classifier
n_model = GaussianNB()

# Train the model using the training sets
n_model.fit(X_train,y_train)

#Predict Output
predicted= n_model.predict(X_test) # # 예측된 클래스 목록을 반환함 - 모든 데이터 포인트에 대해 하나의 예측
probabilities = n_model.predict_proba(X_test) # 모든 데이터 포인트에 대해 각 클래스의 확률 목록을 반환합니다.
print("Predicted Value:", predicted)
print("Predicted percent:", probabilities)

n_model.score(X_train, y_train)

n_model.score(X_test, y_test)

from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
accuracy = accuracy_score(y_test, predicted)
recall = recall_score(y_test, predicted,average="macro")
precision = precision_score(y_test, predicted,average="macro")
f1 = f1_score(y_test, predicted,average="macro")
print('정확도:{0}, 재현율:{1}, 정밀도{2}, f1 score{3}'.format(accuracy, recall, precision, f1))

"""# K-NN"""

from sklearn.neighbors import KNeighborsClassifier
k_model = KNeighborsClassifier()

k_model.fit(X_train,y_train)

predicted= k_model.predict(X_test)
probabilities = k_model.predict_proba(X_test)
print("Predicted Value:", predicted)
print("Predicted percent:", probabilities)

n_model.score(X_train, y_train)

n_model.score(X_test, y_test)

from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
accuracy = accuracy_score(y_test, predicted)
recall = recall_score(y_test, predicted,average="macro")
precision = precision_score(y_test, predicted,average="macro")
f1 = f1_score(y_test, predicted,average="macro")
print('정확도:{0}, 재현율:{1}, 정밀도{2}, f1 score{3}'.format(accuracy, recall, precision, f1))

"""# **2. 외계행성 찾기**"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score

train_df = pd.read_csv('exoTrain.csv')

train_df.head()

train_df.isnull().sum().sum()

train_df.shape

train_df.describe()

X = train_df.drop('LABEL', axis=1)
y = train_df['LABEL']

clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)

clf.fit(X, y)

scores = cross_val_score(clf, X, y, cv=5)

print("Accuracy: %0.2f%%" % (scores.mean()*100))

# test

test_df = pd.read_csv('exoTest.csv')

test_df.isnull().sum().sum()

test_df.shape

test_df.describe()

X_test = test_df.drop('LABEL', axis=1)
y_test = test_df['LABEL']

y_pred_test = clf.predict(X)

accuracy = accuracy_score(y, y_pred_test)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

"""# **3. 외계행성 거주 가능성 출력하기**"""

pd.set_option('display.max_columns', None)
working_data.head()

x = [list(input("'P_TYPE_TEMP','P_PERIOD','S_DEC','S_DISTANCE','S_MASS','S_TEMPERATURE','P_TYPE','S_TIDAL_LOCK','P_HABZONE_OPT','P_RADIUS_EST' 의 값을 입력하세요: ").split(','))]
#from sklearn.preprocessing import LabelEncoder
#lencoders1 = {}
type(x)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x= scaler.fit_transform(x)

predicted_x= k_model.predict(x)
probabilities_x = k_model.predict_proba(x)
print("Predicted Value:", predicted_x)
print("Predicted percent:", probabilities_x)